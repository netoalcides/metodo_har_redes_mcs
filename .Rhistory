horizons <- 5
rolling_valid_sample <- 1
har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = 5 )
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = 5 )
horizons <- 5
zz <- foreach( rolling_valid_sample = 1350:( round( (T-T_training)/horizons) ), .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
# keep forecasts in a data_frame
bind_rows( error_results,
forecasts %>%
mutate( error = prediction - rv5_252 ) )
}
# forecasting 1, 5, 10, 15 days ahead
registerDoFuture()
plan(multiprocess)
horizons <- 5
error_results <- NULL
zz <- foreach( rolling_valid_sample = 1350:( round( (T-T_training)/horizons) ), .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
# keep forecasts in a data_frame
bind_rows( error_results,
forecasts %>%
mutate( error = prediction - rv5_252 ) )
}
har_classic_forecast
horizons <- 5
error_results <- NULL
rolling_valid_sample <- 1
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
data_to_loop %>% tail
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
forecasts
# keep forecasts in a data_frame
bind_rows( error_results,
forecasts %>%
mutate( error = prediction - rv5_252 ) )
error_results <- NULL
zz <- foreach( rolling_valid_sample = 1:5, .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
# keep forecasts in a data_frame
bind_rows( error_results,
forecasts %>%
mutate( error = prediction - rv5_252 ) )
}
zz
zz %>% print(n=Inf)
zz <- foreach( rolling_valid_sample = 1:5, .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
}
zz %>% print(n=Inf)
( round( (T-T_training)/horizons) )
zz <- foreach( rolling_valid_sample = 260:( round( (T-T_training)/horizons) ), .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
}
zz
zz %>% print(n=Inf)
foreach( horizons = c(1, 5, 10, 15) ) %:%
foreach( rolling_valid_sample = 1:5, .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
}
## load initial parametros from project
library(ProjectTemplate)
load.project()
ls()
# training
fixed_window_har_original_model <- lm( formula = rv5_252 ~ .,
data =  har_original_data_structure_train %>%
select( -date ) )
# forecasting 1, 5, 10, 15 days ahead
registerDoFuture()
plan(multiprocess)
results_forecasts_har_classic <- foreach( horizons = c(1, 5, 10, 15) ) %:%
foreach( rolling_valid_sample = 1:( round( (T-T_training)/horizons) ), .combine='rbind' ) %dopar% {
# prepare data to forecasts
data_to_loop <- har_original_data_structure %>%
slice( (T_training - 21):(T_training + ( (rolling_valid_sample-1)*horizons ) + 1 ) ) %>%
select( date, rv5_252 )
# loop recursive forecasts
forecasts <- har_classic_forecast( model = fixed_window_har_original_model,
x_reg = data_to_loop,
horizons = horizons )
}
results_forecasts_har_classic
h1 <- results_forecasts_har_classic[[1]]
h1
h5 <- results_forecasts_har_classic[[2]]
h10 <- results_forecasts_har_classic[[3]]
h15 <- results_forecasts_har_classic[[4]]
h1
h5
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 )
h5 %>% tail
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
tail
h10 %>% tail
h10 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
tail
h15 %>% tail
h15 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
tail
bind_cols( h1,
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
mutate( prediction_h5 = prediction ) %>%
select(prediction_h5) )
bind_cols( h1,
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
mutate( prediction_h5 = prediction ) %>%
select(prediction_h5) )
bind_cols( h1 %>%
rename( prediction_h1 = prediction ),
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h5 = prediction ) %>%
select(prediction_h5) )
bind_cols( h1 %>%
rename( prediction_h1 = prediction ),
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h5 = prediction ) %>%
select(prediction_h5),
h10 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h10 = prediction ) %>%
select(prediction_h10),
h15 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h15 = prediction ) %>%
select(prediction_h15) )
bind_cols( h1 %>%
rename( prediction_h1 = prediction ),
h5 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h5 = prediction ) %>%
select(prediction_h5),
h10 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h10 = prediction ) %>%
select(prediction_h10),
h15 %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h15 = prediction ) %>%
select(prediction_h15) ) %>%
summarise( erro_1 = sqrt( mean( ( prediction_h1 - rv5_252 )^2 ) ),
erro_5 = sqrt( mean( ( prediction_h5 - rv5_252 )^2 ) ),
erro_10 = sqrt( mean( ( prediction_h10 - rv5_252 )^2 ) ),
erro_15 = sqrt( mean( ( prediction_h15 - rv5_252 )^2 ) ) )
results_forecasts_har_classic_by_horizon <- bind_cols( results_forecasts_har_classic[[1]] %>%
rename( prediction_h1 = prediction ),
results_forecasts_har_classic[[2]] %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h5 = prediction ) %>%
select(prediction_h5),
results_forecasts_har_classic[[3]] %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h10 = prediction ) %>%
select(prediction_h10),
results_forecasts_har_classic[[4]] %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h15 = prediction ) %>%
select(prediction_h15) )
fixed_window.results_forecasts_har_classic_by_horizon <- bind_cols( results_forecasts_har_classic[[1]] %>%
rename( prediction_h1 = prediction ),
results_forecasts_har_classic[[2]] %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h5 = prediction ) %>%
select(prediction_h5),
results_forecasts_har_classic[[3]] %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h10 = prediction ) %>%
select(prediction_h10),
results_forecasts_har_classic[[4]] %>%
group_by( date ) %>%
filter( row_number() <= 1 ) %>%
ungroup() %>%
rename( prediction_h15 = prediction ) %>%
select(prediction_h15) )
cache("fixed_window.results_forecasts_har_classic_by_horizon")
fixed_window.results_forecasts_har_classic_by_horizon
ls()
rm( results_forecasts_har_classic )
## load initial parametros from project
library(ProjectTemplate)
load.project()
ls()
fixed_window.results_forecasts_har_classic_by_horizon
# Habilita o pacote quantmod
library(quantmod)
#Cria um novo ambiente para armazenar os dados
stockData <- new.env()
#Especifica as datas de interesse
startDate = as.Date("2011-01-01")
endDate = as.Date("2011-12-31")
#Obtêm os dados do ativo PETR4 e PETR3
getSymbols("PETR4.SA",
src = "yahoo",
from = startDate,
to = endDate)
ls()
#Calcula o log-retorno
retorno <- na.omit( diff( log( Cl( PETR4.SA ) ) ) )
retorno
ls()
rm(PETR4.SA)
# metdo NFHS (Normal Filtered Historical Simulation)
library(fGarch)
install.packages("fGarch")
# metdo NFHS (Normal Filtered Historical Simulation)
library(fGarch)
lambda <- 0.05
# Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno, order=c(1,0,0) ) # estima model ARIMA
fit
summary(fit)
modelo.N.FHS <- garchFit( ~garch(1, 1), fit.res , cond.dist = "norm")
fit.res <- resid(fit) # obtem os residuos
fit.res
modelo.N.FHS <- garchFit( ~garch(1, 1), fit.res , cond.dist = "norm")
modelo.N.FHS
modelo.N.FHS@residuals
modelo.N.FHS@sigma.t
fit.res
sresi <- (modelo.N.FHS@residuals / modelo.N.FHS@sigma.t)
sresi
predict(fit, n.ahead = 1)
predict(fit, n.ahead = 1)$pred
predict(fit, n.ahead = 1)$pred[1]
predict(modelo.N.FHS , n.ahead = 1)
[1,1]
predict(modelo.N.FHS , n.ahead = 1)[1,1]
sigma.t1 <- predict(modelo.N.FHS , n.ahead = 1)[1,1]
length(sresi)
runif(1000)
runif(1000)*length(sresi)
ceiling( runif(1000)*length(sresi) )
sigma.t1
sresi[uniforme]
#Passo3: Para o período t+1 gera-se um conjunto de Retornos
uniforme <- ceiling( runif(1000)*length(sresi) )
uniforme
mu.t1
#Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead = 1)$pred[1]
sigma.t1 <- predict(modelo.N.FHS , n.ahead = 1)[1,1]
mu.t1
sigma.t1
Retornos <- (sresi[uniforme]*sigma.t1) + mu.t1
Retornos
#Passo4: Calcular o VaR
VaR.t1 <- -quantile(Retornos, lambda)
VaR.t1
lambda
-quantile(Retornos, 0.01)
-quantile(Retornos, lambda)
# Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno,order=c(1,0,0) )
fit.res <- resid(fit)
modelo.S.FHS <- garchFit( ~garch(1, 1),
fit.res,
cond.dist = "sstd" )
sresi<-(modelo.S.FHS@residuals/modelo.S.FHS@sigma.t)
# Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead=1)$pred[1]
sigma.t1 <- predict(modelo.S.FHS, n.ahead=1)[1,1]
mu.t1
sigma.t1
#Passo3: Para o período t+1 gera-se um conjunto de Retornos
uniforme <- ceiling( runif(1000)*length(sresi) )
Retornos <- (sresi[uniforme]*sigma.t1) + mu.t1
Retornos
#Passo4: Calcular o VaR
VaR.t1.S.FHS <- -quantile(Retornos, lambda)
VaR.t1.S.FHS
VaR.t1
# Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno, order=c(1,0,0) )
fit.res <- resid(fit)
modelo.S.FHS <- garchFit( ~garch(1, 1),
fit.res,
cond.dist = "sstd" )
sresi <- (modelo.S.FHS@residuals/modelo.S.FHS@sigma.t)
# Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead=1)$pred[1]
sigma.t1 <- predict(modelo.S.FHS, n.ahead=1)[1,1]
#Passo3: Para o período t+1 gera-se um conjunto de Retornos
uniforme <- ceiling( runif(1000)*length(sresi) )
Retornos <- (sresi[uniforme]*sigma.t1) + mu.t1
#Passo4: Calcular o VaR
VaR.t1.S.FHS <- -quantile(Retornos, lambda)
VaR.t1.S.FHS
VaR.t1
#Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno, order=c(1,0,0) )
fit.res <- resid(fit)
modelo.N.EVT <- garchFit( ~garch(1, 1),
fit.res,
cond.dist = "norm" )
sresi <- ( modelo.N.EVT@residuals/modelo.N.EVT@sigma.t )
sresi
#Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead=1)$pred[1]
sigma.t1 <- predict(modelo.N.EVT , n.ahead=1)[1,1]
#Passo3: Estima o quantil com base na distribuição GEV
library(fExtremes)
install.packages("fExtremes")
#Passo3: Estima o quantil com base na distribuição GEV
library(fExtremes)
sresi
-sresi
sresi.menos <- -sresi
MLE <- gevFit( sresi.menos, type="pwm" )
MLE
MLE@fit$par.ests
xi <- MLE@fit$par.ests[1]
mu <- MLE@fit$par.ests[2]
sigma <- MLE@fit$par.ests[3]
#Passo4: Calcular o VaR
quantil <- qgev(lambda, xi = xi, mu = mu, beta = sigma, lower.tail = TRUE)
quantil
quantil[1]
VaR.t1.N.EVT <- -(mu.t1 + (quantil[1]*sigma.t1) )
VaR.t1.N.EVT
VaR.t1.S.FHS
VaR.t1
#Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno, order=c(1,0,0) )
fit.res <- resid(fit)
modelo.ST.EVT <- garchFit( ~garch(1, 1),
fit.res,
cond.dist = "sstd" )
sresi <- (modelo.ST.EVT@residuals/modelo.ST.EVT@sigma.t)
#Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead=1)$pred[1]
sigma.t1 <- predict(modelo.ST.EVT, n.ahead=1)[1,1]
sresi.menos <- -sresi
MLE <- gevFit( sresi.menos, type="pwm")
xi <- MLE@fit$par.ests[1]
mu <- MLE@fit$par.ests[2]
sigma <- MLE@fit$par.ests[3]
#Passo4: Calcular o VaR
quantil <- qgev(lambda, xi = xi, mu = mu, beta = sigma, lower.tail = TRUE)
VaR.t1.ST.EVT <- -(mu.t1 + (quantil[1]*sigma.t1) )
VaR.t1.ST.EVT
# Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno, order=c(1,0,0) ) # estima model ARIMA
fit.res <- resid(fit) # obtem os residuos
modelo.N.FHS <- garchFit( ~garch(1, 1),
fit.res,
cond.dist = "norm" )
sresi <- (modelo.N.FHS@residuals / modelo.N.FHS@sigma.t)
#Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead = 1)$pred[1]
sigma.t1 <- predict(modelo.N.FHS , n.ahead = 1)[1,1]
#Passo3: Para o período t+1 gera-se um conjunto de Retornos
uniforme <- ceiling( runif(1000)*length(sresi) )
Retornos <- (sresi[uniforme]*sigma.t1) + mu.t1
#Passo4: Calcular o VaR
VaR.t1.N.FHS <- -quantile(Retornos, lambda)
VaR.t1.N.FHS
# compara os metodos
VaR.t1.N.FHS
VaR.t1.S.FHS
# compara os metodos
VaR.t1.N.FHS
VaR.t1.S.FHS
VaR.t1.N.EVT
VaR.t1.ST.EVT
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS,
VaR.t1.S.FHS = VaR.t1.S.FHS )
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS,
VaR.t1.S.FHS = VaR.t1.S.FHS,
VaR.t1.N.EVT = VaR.t1.N.EVT,
VaR.t1.ST.EVT = VaR.t1.ST.EVT )
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS * 100,
VaR.t1.S.FHS = VaR.t1.S.FHS * 100,
VaR.t1.N.EVT = VaR.t1.N.EVT * 100,
VaR.t1.ST.EVT = VaR.t1.ST.EVT * 100 )
#Passo1: Encotrando os resíduos padronizados
fit <- arima( retorno, order=c(1,0,0) )
fit.res <- resid(fit)
modelo.S.EVT <- garchFit( ~garch(1, 1),
fit.res,
cond.dist = "sstd" )
sresi <- (modelo.S.EVT@residuals/modelo.S.EVT@sigma.t)
#Passo2: Obtem-se sigma_t+1 e mu_t+1
mu.t1 <- predict(fit, n.ahead=1)$pred[1]
sigma.t1 <- predict(modelo.S.EVT, n.ahead=1)[1,1]
#Passo3: Estima o quantil com base na distribuição GEV
library(fExtremes)
sresi.menos <- -sresi
MLE <- gevFit( sresi.menos, type="pwm")
xi <- MLE@fit$par.ests[1]
mu <- MLE@fit$par.ests[2]
sigma <- MLE@fit$par.ests[3]
#Passo4: Calcular o VaR
quantil <- qgev(lambda, xi = xi, mu = mu, beta = sigma, lower.tail = TRUE)
VaR.t1.S.EVT <- -(mu.t1 + (quantil[1]*sigma.t1) )
# compara os metodos
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS * 100,
VaR.t1.S.FHS = VaR.t1.S.FHS * 100,
VaR.t1.N.EVT = VaR.t1.N.EVT * 100,
VaR.t1.S.EVT = VaR.t1.S.EVT * 100 )
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS * 100,
VaR.t1.S.FHS = VaR.t1.S.FHS * 100,
VaR.t1.N.EVT = VaR.t1.N.EVT * 100,
VaR.t1.S.EVT = VaR.t1.S.EVT * 100 ) %>%
gather( key = metodos, value = VaR )
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS * 100,
VaR.t1.S.FHS = VaR.t1.S.FHS * 100,
VaR.t1.N.EVT = VaR.t1.N.EVT * 100,
VaR.t1.S.EVT = VaR.t1.S.EVT * 100 ) %>%
gather( key = metodos, value = VaR ) %>%
ggplot( aes( x = metodos, y = VaR) ) +
geom_bar()
data_frame( VaR.t1.N.FHS = VaR.t1.N.FHS * 100,
VaR.t1.S.FHS = VaR.t1.S.FHS * 100,
VaR.t1.N.EVT = VaR.t1.N.EVT * 100,
VaR.t1.S.EVT = VaR.t1.S.EVT * 100 ) %>%
gather( key = metodos, value = VaR ) %>%
ggplot( aes( x = metodos, y = VaR) ) +
geom_bar( stat = "identity" )
